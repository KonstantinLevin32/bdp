VEROSE: False
BASE_TASK_CONFIG_PATH: bdp/config/tasks/base.yaml
TRAINER_NAME: "ddppo"
ENV_NAME: "RearrangeRLEnv"
SIMULATOR_GPU_ID: 0
VERBOSE: False
TORCH_GPU_ID: 0
VIDEO_OPTION: ["disk"]
TENSORBOARD_DIR: "tb"
VIDEO_DIR: "video_dir"
VIDEO_FPS: 30
VIDEO_RENDER_TOP_DOWN: False
VIDEO_RENDER_ALL_INFO: True
TEST_EPISODE_COUNT: -1
EVAL_CKPT_PATH_DIR: null
NUM_ENVIRONMENTS: 32
WRITER_TYPE: 'tb'
# Visual sensors to include
SENSORS: ["HEAD_DEPTH_SENSOR"]
CHECKPOINT_FOLDER: "data/new_checkpoints"
NUM_UPDATES: -1
TOTAL_NUM_STEPS: 5.0e7
LOG_INTERVAL: 5
NUM_CHECKPOINTS: 20
FORCE_TORCH_SINGLE_THREADED: True
EVAL_KEYS_TO_INCLUDE_IN_NAME: ['composite_success']
EVAL:
  SPLIT: "val"
  USE_CKPT_CONFIG: False
  SHOULD_LOAD_CKPT: True
TASK_CONFIG:
  TASK:
    SENSORS:
      - TARGET_START_SENSOR
      - GOAL_SENSOR
      - JOINT_SENSOR
      - IS_HOLDING_SENSOR
      - END_EFFECTOR_SENSOR
      - RELATIVE_RESTING_POS_SENSOR
      - TARGET_START_GPS_COMPASS_SENSOR
      - TARGET_GOAL_GPS_COMPASS_SENSOR
      - LOCALIZATION_SENSOR 
      - REL_POS_SENSOR
      - COLL_INFO
      - GLOBAL_PREDICATE_SENSOR
      - MULTI_ROBOT_REL_OBJ_POS

  SIMULATOR:
    HEAD_DEPTH_SENSOR:
        WIDTH: 2
        HEIGHT: 2
RL:
  MA_VIS:
    # This is for rendering the action timelines.
    LOG_INTERVAL: -1

  AGENT_TRACKER:
    LOG_INTERVAL: -1
    LOG_KEYS:
      - "count"
      - "composite_success"
    RENAME_MAP:
      "composite_success": "Success"
      "stage_bowl_2": "Open Cab"
      "stage_bowl_3": "Pick Bowl"
      "stage_bowl_5": "Place Bowl"
      "stage_fruit_2": "Open Fridge"
      "stage_fruit_3": "Pick Fruit"
      "stage_fruit_5": "Place Fruit"

    # Plotting options
    CMAP: "Reds"
    SUCC_CMAP: "Blues"
    EVENT_NAME: "Event (E)"
    BLAME_NAME: "$\\mathbb{P}(E = 1)$"

    IGNORE_SUBGOALS: ['bowl_1', 'bowl_4', 'fruit_1', 'fruit_4']
    RENDER_SELF: False
    WINDOW_SIZE: 10
    TRAJ_WINDOW_SIZE: -1
    POINTS_WINDOW_SIZE: -1
  AGENT_SAMPLER: 
    TYPE: "PopulationPlayAgentSampler"
    SELF_PLAY: False
    SAMPLE_INTERVAL: 10
    NUM_AGENTS: 2
    # The number of updates between checkpoint intervals
    NUM_SAMPLE_AGENTS: 2
    SECOND_STAGE_START: -1.0
    ALLOW_SELF_SAMPLE: True
    ONLY_SELF_SAMPLE: False
    SINGLE_UPDATE: False
    FIX_AGENT_A: -1
    FIX_AGENT_B: -1

    # Pop play params
    REUSE_VISUAL_ENCODER: True
    FORCE_CPU: False
    NUM_AGENTS: 2
    SELF_PLAY: False

    # Pref play params
    PREF_DIM: 16

    LOAD_POP_CKPT: ""

    # Evaluation 
    EVAL_CKPT_A: ""
    EVAL_CKPT_B: ""
    EVAL_IDX_A: 0
    EVAL_IDX_B: 0
    AGENT_A_NAME: ""
    AGENT_B_NAME: ""

  POLICY:
    OBS_TRANSFORMS:
      ENABLED_TRANSFORMS: ['AddVirtualKeys']
      ADD_VIRTUAL_KEYS:
        "nav_to_skill": 8
        "object_to_agent_gps_compass": 2
        "marker_rel_pos": 3
        "marker_js": 2
  POLICIES:
    POLICY_0:
      robot_id: "AGENT_0"
      name: "TrainableHierarchicalPolicy"
      should_call_stop: False
      batch_dup: 1
      high_level_policy:
        name: "NnHighLevelPolicy"
        use_rnn: False
        use_normalized_advantage: False
        min_batch_size: -1
        replan_dist: 1.0
        replan_once: False
        allow_other_place: False

        PREF_DIM: -1
        OTHER_PREF_DIM: -1
        N_AGENTS: -1

        # Pref discrim
        use_pref_discrim: False
        pref_discrim:
          in_keys: 
            - "localization_sensor"
          hidden_dim: 128
          lr: 0.0003
          reward_weight: 1.0

        use_aux_pred: False
        aux_pred:
          weight: 1.0

        use_obs_keys: 
          - "all_predicates"
          - "is_holding"
          - "rel_pos_sensor"
          - "multi_robot_rel_obj_pos"

        hidden_dim: 128
        n_recurrent_layers: 2
        rnn_type: 'LSTM'

        solution: []
  SKILL_OVERRIDES: {}
  DEFINED_SKILLS:
    NN_OPEN_CAB:
      skill_name: "ArtObjSkillPolicy"
      name: "PointNavResNetPolicy"
      action_distribution_type: "gaussian"
      AT_RESTING_THRESHOLD: 0.15
      OBS_SKILL_INPUTS: []
      LOAD_CKPT_FILE: "data/models/open_cab.pth"
      MAX_SKILL_STEPS: 200
      START_ZONE_RADIUS: 0.3
      FORCE_END_ON_TIMEOUT: False
      MAX_SKILL_STEPS: 1
      skill_name: "NoopSkillPolicy"
      apply_postconds: True

    # NN_CLOSE_CAB:
    #   skill_name: "ArtObjSkillPolicy"
    #   name: "PointNavResNetPolicy"
    #   action_distribution_type: "gaussian"
    #   AT_RESTING_THRESHOLD: 0.15
    #   OBS_SKILL_INPUTS: []
    #   LOAD_CKPT_FILE: "data/models/close_cab.pth"
    #   MAX_SKILL_STEPS: 200
    #   START_ZONE_RADIUS: 0.3
    #   FORCE_END_ON_TIMEOUT: False
    #   MAX_SKILL_STEPS: 1
    #   skill_name: "NoopSkillPolicy"
    #   apply_postconds: True

    NN_OPEN_FRIDGE:
      skill_name: "ArtObjSkillPolicy"
      name: "PointNavResNetPolicy"
      action_distribution_type: "gaussian"
      AT_RESTING_THRESHOLD: 0.15
      OBS_SKILL_INPUTS: []
      LOAD_CKPT_FILE: "data/models/open_fridge.pth"
      MAX_SKILL_STEPS: 200
      START_ZONE_RADIUS: 0.3
      FORCE_END_ON_TIMEOUT: False
      MAX_SKILL_STEPS: 1
      skill_name: "NoopSkillPolicy"
      apply_postconds: True

    # NN_CLOSE_FRIDGE:
    #   skill_name: "ArtObjSkillPolicy"
    #   name: "PointNavResNetPolicy"
    #   action_distribution_type: "gaussian"
    #   AT_RESTING_THRESHOLD: 0.15
    #   OBS_SKILL_INPUTS: []
    #   LOAD_CKPT_FILE: "data/models/close_fridge.pth"
    #   MAX_SKILL_STEPS: 200
    #   START_ZONE_RADIUS: 0.3
    #   FORCE_END_ON_TIMEOUT: False
    #   MAX_SKILL_STEPS: 1
    #   skill_name: "NoopSkillPolicy"
    #   apply_postconds: True

    NN_PICK:
      skill_name: "PickSkillPolicy"
      name: "PointNavResNetPolicy"
      action_distribution_type: "gaussian"
      AT_RESTING_THRESHOLD: 0.15
      OBS_SKILL_INPUTS: ["obj_start_sensor"]
      LOAD_CKPT_FILE: "data/models/pick.pth"
      MAX_SKILL_STEPS: 200
      FORCE_END_ON_TIMEOUT: False
      MAX_SKILL_STEPS: 1
      skill_name: "NoopSkillPolicy"
      apply_postconds: True

    NN_PLACE:
      skill_name: "PlaceSkillPolicy"
      name: "PointNavResNetPolicy"
      action_distribution_type: "gaussian"
      AT_RESTING_THRESHOLD: 0.15
      OBS_SKILL_INPUTS: ["obj_goal_sensor"]
      LOAD_CKPT_FILE: "data/models/place.pth"
      MAX_SKILL_STEPS: 200
      FORCE_END_ON_TIMEOUT: False
      MAX_SKILL_STEPS: 1
      skill_name: "NoopSkillPolicy"
      apply_postconds: True

    GT_NAV:
      skill_name: "OracleNavPolicy"
      OBS_SKILL_INPUTS: ["obj_start_sensor", "abs_obj_start_sensor", "obj_goal_sensor", "abs_obj_goal_sensor"]
      GOAL_SENSORS: ["obj_goal_sensor", "abs_obj_goal_sensor"]
      NAV_ACTION_NAME: "BASE_VELOCITY"
      MAX_SKILL_STEPS: 300
      FORCE_END_ON_TIMEOUT: True
      STOP_THRESH: 0.001
      apply_postconds: True

    NOOP_SKILL:
      skill_name: "WaitSkillPolicy"
      FORCE_END_ON_TIMEOUT: False
      MAX_SKILL_STEPS: 10
      skill_name: "NoopSkillPolicy"
      apply_postconds: True

    TURN_LEFT_SKILL:
      skill_name: "TurnSkillPolicy"
      FORCE_END_ON_TIMEOUT: False
      MAX_SKILL_STEPS: 5
      TURN_STEPS: 1
      TURN_POWER: 1.0
      apply_postconds: True

    TURN_RIGHT_SKILL:
      skill_name: "TurnSkillPolicy"
      FORCE_END_ON_TIMEOUT: False
      MAX_SKILL_STEPS: 5
      TURN_STEPS: 1
      TURN_POWER: -1.0
      apply_postconds: True

    WAIT_SKILL:
      skill_name: "WaitSkillPolicy"
      MAX_SKILL_STEPS: -1
      FORCE_END_ON_TIMEOUT: False
      MAX_SKILL_STEPS: 1
      skill_name: "NoopSkillPolicy"
      apply_postconds: True

    RESET_ARM_SKILL:
      skill_name: "ResetArmSkill"
      MAX_SKILL_STEPS: 50
      RESET_JOINT_STATE: [-4.5003259e-01, -1.0799699e00, 9.9526465e-02, 9.3869519e-01, -7.8854430e-04, 1.5702540e00, 4.6168058e-03]
      FORCE_END_ON_TIMEOUT: False
      MAX_SKILL_STEPS: 1
      skill_name: "NoopSkillPolicy"
      apply_postconds: True

  USE_SKILLS:
    open_cab: "NN_OPEN_CAB"
    noop: "NOOP_SKILL"
    turn_left: "TURN_LEFT_SKILL"
    turn_right: "TURN_RIGHT_SKILL"
    # close_cab: "NN_CLOSE_CAB"
    open_fridge: "NN_OPEN_FRIDGE"
    # close_fridge: "NN_CLOSE_FRIDGE"
    pick: "NN_PICK"
    place: "NN_PLACE"
    nav: "GT_NAV"
    nav_to_receptacle: "GT_NAV"
    reset_arm: "RESET_ARM_SKILL"
    wait: "WAIT_SKILL"


  PPO:
    # ppo params
    clip_param: 0.2
    ppo_epoch: 2
    num_mini_batch: 1
    value_loss_coef: 0.5
    entropy_coef: 1e-4
    lr: 1e-3
    eps: 1e-5
    max_grad_norm: 0.2
    num_steps: 128
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: False
    reward_window_size: 50

    use_normalized_advantage: False

    hidden_size: 2

    # Use double buffered sampling, typically helps
    # when environment time is similar or large than
    # policy inference time during rollout generation
    use_double_buffered_sampler: False

  DDPPO:
    sync_frac: 0.6
    # The PyTorch distributed backend to use
    distrib_backend: NCCL
    # Visual encoder backbone
    pretrained_weights: data/ddppo-models/gibson-2plus-resnet50.pth
    # Initialize with pretrained weights
    pretrained: False
    # Initialize just the visual encoder backbone with pretrained weights
    pretrained_encoder: False
    # Whether or not the visual encoder backbone will be trained.
    train_encoder: True
    # Whether or not to reset the critic linear layer
    reset_critic: False

    # Model parameters
    backbone: resnet50
    rnn_type: LSTM
    num_recurrent_layers: 2
